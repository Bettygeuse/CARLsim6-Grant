/*!

\page tut7_pti Tutorial 7: Parameter Tuning Interface (PTI)
\tableofcontents
\author Kristofor D. Carlson


\section tut7s1_intro 7.1 Introduction

In this tutorial we introduce the parameter tuning interface and how to tune the weight ranges of a
simple SNN to achieve a particular target firing rate. We will be tuning four parameter values that
represent the weight ranges, and we will use the 
<a href="https://cs.gmu.edu/~eclab/projects/ecj/">ECJ evolutionary computation framework</a>.

We will accomplish this by place our CARLsim model inside a program that accept parameter values on 
`stdin`, runs the model, and then rights fitness values to `stdout`.  This provides an interface that 
allows your CARLsim model to be tuned by external optimization tools such as ECJ.

All the required files are found in the doc/source/tutorial/7_pti/src/ directory. The user is encouraged
to follow along and experiment with the source code found there.

This tutorial assumes:
- ECJ version 28 or greater is installed.  Find it at https://cs.gmu.edu/~eclab/projects/ecj/
- CARLsim 5.0 is installed.

The overview of parameter tuning is as follows. ECJ handles all steps of the evolutionary algorithm
(EA) except for \em fitness \em evaluation which is done by CARLsim through the implementation of an
experiment class function called <tt>run</tt>.

A more in-depth overview of CARLsim and ECJ can be found in \ref ch10_ecj.


\section tut7s3_experiment_class 7.2 The Experiment Class

Let's begin by building our CARLsim model.  The full example is found in <tt>main_TuneFiringRates.cpp</tt>.  You can 
use it as a template for implementing your own parameter-tuning experiments.

We'll do three things a little differently from the previous tutorials:

 1. we'll wrap our model in a CLI that accepts parameters on `stdin`,
 2. we'll use the `ParameterInstances` class to handles parsing the input parameters and plug them into our model, and
 3. after the model runs, we'll write a fitness value to `stdout` for each parameter instance we received.


\subsection The Main Method

For the high-level part of our program, we'll start by including CARLsim and `PTI.h`.  The latter cointains some high-level
code that helps us standardize our parameter-tuning interface across projects (namely the `Experiment` and `PTI` classes).

Then for our `main`, we'll begin by loading a couple of CLI parameters.  The `verbosity` parameter in particular will allow us
to write CARLsim logs to stdout while we're debugging our model, but to keep them silent while tuning (so only fitness 
values are written to `stdout`):

\code
#include <iostream>
#include <carlsim.h>
#include "PTI.h"

int main(int argc, char* argv[]) {
	const SimMode simMode = hasOpt(argc, argv, "cpu") ? CPU_MODE : GPU_MODE;
  	const LoggerMode verbosity = hasOpt(argc, argv, "v") ? USER : SILENT;
\endcode

Next, we'll instantiate our model.  We'll write the impelmentation of `TunefiringRatesExperiment` class in
a moment below:

\code
	const TuneFiringRatesExperiment experiment(simMode, verbosity);
\endcode

To execute the model, we'll use the `PTI` class.  This is a simple class that orchestrates the loading of parameters from
an input stream (usually `std::cin`), uses them to execute your `Experiment` subclass, and collects the results to an 
output stream (`std::cout`).

\code
	const PTI pti(argc, argv, std::cout, std::cin);

	pti.runExperiment(experiment);

	return 0;
}
\endcode


\subsection The Experiment Class

Now, at the top of the same file, we'll implement the `TuneFiringRatesExperiment` class that
our `main()` uses.

The layout of the SNN is as follows. We have 10 input excitatory
neurons, 10 regular spiking (RS) excitatory neurons that receive this input, and 10 fast spiking
(FS) izhikevich neurons that receive input from the excitatory RS neurons. Additionally, the
inhibitory group is connected to the RS excitatory group and the RS excitatory group is connected to
itself recurrently. There are therefore 4 connection weight ranges to be tuned. The goal of the
tuning is make the RS excitatory group have an average firing rate of 10 Hz and make the FS
inhibitory group have an average firing rate of 20 Hz.

First we'll inhereit from the `Experiment` base class and define some constants used by our model.
This makes it possible to pass our model into into a `pti.runExperiment()` call. specific experiment
class must be inherited from the base class and default constructor must be defined.:

\code
class TuneFiringRatesExperiment : public Experiment {
public:
	// Decay constants
	static const float COND_tAMPA=5.0, COND_tNMDA=150.0, COND_tGABAa=6.0, COND_tGABAb=150.0;
	
	// Number of neurons in each group
	static const int NUM_NEURONS_PER_GROUP = 100;

	// Simulation time in seconds
	static const int runTime = 2;

	// Target rates for the objective function
	static const float INPUT_TARGET_HZ = 30.0f;
	static const float EXC_TARGET_HZ   = 10.0f;
	static const float INH_TARGET_HZ   = 20.0f;
\endcode

We'll also add two member variables so we can remember the loggins and cpu-vs.-gpu configuration.
All our constructor will do is set these two values:

\code
    const LoggerMode verbosity;
	const SimMode simMode;

	TuneFiringRatesExperiment(const SimMode simMode, const LoggerMode verbosity): simMode(simMode), verbosity(verbosity) {}
\endcode






























After that, the user should only be concerned with writing the <tt>run</tt> member function.

\code
...
void run(const ParameterInstances &parameters, std::ostream &outputStream) const {
	int indiNum = parameters.getNumInstances();

	int poissonGroup[indiNum];
	int excGroup[indiNum];
	int inhGroup[indiNum];
	SpikeMonitor* excMonitor[indiNum];
	SpikeMonitor* inhMonitor[indiNum];
	float excHz[indiNum];
	float inhHz[indiNum];
	float excError[indiNum];
	float inhError[indiNum];
	float fitness[indiNum];
...
\endcode

Notice we can use the parameters object to query how many instances/individuals we have defined in
the ECJ parameter file. Now that we have that value, we manually create arrays neuron groups of size
<tt>indiNum</tt>. In our case we have defined 10 individuals per generation.

Next we create our CARLsim network object and assign the 4 parameter values generated by ECJ to our
10 SNN individuals. We iterate over all 10 individuals and assign them 4 distinct parameters each.

\code
CARLsim* const network = new CARLsim("tuneFiringRatesECJ", GPU_MODE, SILENT);

for(unsigned int i = 0; i < parameters.getNumInstances(); i++) {
	poissonGroup[i] = network->createSpikeGeneratorGroup("poisson", NUM_NEURONS, EXCITATORY_NEURON);
	excGroup[i] = network->createGroup("exc", NUM_NEURONS, EXCITATORY_NEURON);
	inhGroup[i] = network->createGroup("inh", NUM_NEURONS, INHIBITORY_NEURON);

	network->setNeuronParameters(excGroup[i], REG_IZH[0], REG_IZH[1], REG_IZH[2], REG_IZH[3]);
	network->setNeuronParameters(inhGroup[i], FAST_IZH[0], FAST_IZH[1], FAST_IZH[2], FAST_IZH[3]);
	network->setConductances(true,COND_tAMPA,COND_tNMDA,COND_tGABAa,COND_tGABAb);

	network->connect(poissonGroup[i], excGroup[i], "random", RangeWeight(parameters.getParameter(i,0)), 0.5f, RangeDelay(1));
	network->connect(excGroup[i], excGroup[i], "random", RangeWeight(parameters.getParameter(i,1)), 0.5f, RangeDelay(1));
	network->connect(excGroup[i], inhGroup[i], "random", RangeWeight(parameters.getParameter(i,2)), 0.5f, RangeDelay(1));
	network->connect(inhGroup[i], excGroup[i], "random", RangeWeight(parameters.getParameter(i,3)), 0.5f, RangeDelay(1));

}
\endcode

We then setup the SNNs, begin recording their spike data with a SpikeMonitor, and run the SNNs.

\code
...
network->setupNetwork();
...
excMonitor[i] = network->setSpikeMonitor(excGroup[i], "/dev/null");
inhMonitor[i] = network->setSpikeMonitor(inhGroup[i], "/dev/null");

excMonitor[i]->startRecording();
inhMonitor[i]->startRecording();
...
network->runNetwork(runTime,0);
\endcode

Finally we stop recording, get the average firing rate of each group and compute a fitness function
which is output to ECJ using standard Linux streams.

\code
excMonitor[i]->stopRecording();
inhMonitor[i]->stopRecording();

excHz[i] = excMonitor[i]->getPopMeanFiringRate();
inhHz[i] = inhMonitor[i]->getPopMeanFiringRate();

excError[i] = fabs(excHz[i] - EXC_TARGET_HZ);
inhError[i] = fabs(inhHz[i] - INH_TARGET_HZ);

fitness[i] = 1/(excError[i] + inhError[i]);
outputStream << fitness[i] << endl;
\endcode



\section tut7s2_parameter_file 7.3 The ECJ Parameter File

We'll start by configuring ECJ to optimize our model's parameter's with a simple evolutionary algorithm.

To do so, we'll write a parameter file.  ECJ uses big, centralized parameter files to coordinate the 
interaction of a number of different algorithmic modules, traditionally ending with the extention 
`.params` (though they are in fact Java `.properties` files).  In this example, we'll implement a (μ, λ)-style 
evolutionary algorithm (i.e., an algorithm that discards the parents at
each generation) that evolves real-valued paramter vectores with Gaussian mutation of each real-valued gene,
one-point crossover, and truncation selection.  You can find the complete example in
<tt>ecj_TuneFiringRates.params</tt>.

The <a href="https://cs.gmu.edu/~eclab/projects/ecj/manual.pdf">ECJ manual</a> is a great resource for 
undertstanding these parameters and how to configure them in more detail.


\subsection Boiler Plate

Create a new `.params` file and open it with an editor.  We'll start by inheriting some boiler plate 
parameters from a parent file at the top of our file:

\code
parent.0 =                                      $ecj_boiler_plate.params
\endcode

The `ecj_bioler_plate` file contains quite a few standard parameter settings that we typically only alter
in special circumstances—it defines things like the Java classes that manages the algorithm
and population state.

Notice there is a <tt>$</tt> before the filename. This indicates that the location of the 
parent file is specified \em relative to the location of our parameter file.


\subsection Problem

Next, and most importantly, we want to tell ECJ to use an external binary program to compute fitness.  
This is done by configuring the `problem` parameter to point to the `CommandProblem` class.  This is 
a special ECJ fitness function that writes parameters to a program's `std::cin` and reads fitness
values back that your model writes to `std::cout`.

While we're at it we'll also define `evalthreads`, which controls how many instances of your model program
will be run in parallel.  In this tutorial we're relying on the internal parallelism of a single CARLsim
instance to evaluate many individuals at once, so we'll leave ECJ's `evalthreads` set to 1.

\code
eval.problem =                                  ec.app.command.CommandProblem
eval.problem.command =                          $main_TuneFiringRates
evalthreads = 				                    1
\endcode


\subsection Population 

The next important group of parameters controls the population and the outline of how it evolves.
Here we specify that we want a maximum number of 50 generations.  The `quit-on-run-complete`
parameter toggles whether or not evolution should stop early if a solution with the "ideal" 
fitness is found.  We are tuning a noisy fitness function, however, which sometimes causes
an individual to momentarily appear to have a high fitness by chance—and we don't want to 
stop early when that happens.

\code
generations =				                    50
quit-on-run-complete =			                false
\endcode

Now we'll tell ECJ to general 20 individuals to population the initial population (i.e. the
very first generation, when individuals are generated randomly). It's sometimes a good idea 
to create many individuals in the first generation, because this gives us a chance to find a 
good starting point with random search, before we use evolution to refine it.  After that, 
a (μ, λ) breeder takes over evolution, generating `es.lamda.0` children and selecting `es.mu.0` 
parents at each generation.

\code
pop.subpop.0.size =			                    20
breed =					                        ec.es.MuCommaLambdaBreeder
es.mu.0 = 				                        10
es.lambda.0 =                                   20
\endcode

ECJ has several other breeding strategies available, including a (μ + λ) breeder 
(`ec.es.MuPlusLambdaBreeder`), which treats parents and offspring as a combined population 
(so parents have a chance of surviving multiple generations), and `ec.simple.SimpleBreeder`
(which is like `MuCommaLambdaBreeder`, but is meant to work with other traditional selection 
methods, such as tournament selection).


\subsection Representation

Now we'll add several parameters to define how solutions are represented in the EA.  The first 
three are pretty standard, specifcying that we want to use floating-point vectors of numbers:

\code
pop.subpop.0.species =                          ec.vector.FloatVectorSpecies
pop.subpop.0.species.ind =		                ec.vector.DoubleVectorIndividual
pop.subpop.0.species.fitness =		            ec.simple.SimpleFitness
\endcode

Now specify the numer of genes each individual should have, and the ranges within which they
should be initialized.  Our model has four parameters to tune, so we'll want four genes:

\code
pop.subpop.0.species.genome-size =      4
pop.subpop.0.species.min-gene =         0.0005
pop.subpop.0.species.max-gene =         0.5
\endcode

In this case we've configured all four parameters so that they share the same parameter range 
(0.0005, 0.5)—but each individual parameter can be given it's own range if need be 
(see \ref ch10_ecj). 


\subsection Operators

The last algorithmic component we need to configure are the operators.  These define how new 
individuals are created in each generation, and how parents are selected from the previous 
generation.

In ECJ, we add operators by stringing together "pipelines," each of which takes one or more `source`
parameters.  The following lines specify a pipeline that generates individuals via a mutation pipeline,
takes its inputs from a crossover pipeline, which in turn receives individuals (in pairs) from 
an `ESSelection` operator.  `ESSelection` performs truncation selection (i.e. it deterministically
chooses the best individuals in the population). When the algorithm runs, `ESSelection` is applied 
first to the parent population, and mutation occurs last:

\code
pop.subpop.0.species.pipe = 		            ec.vector.breed.VectorMutationPipeline
pop.subpop.0.species.pipe.source.0 = 	        ec.vector.breed.VectorCrossoverPipeline
pop.subpop.0.species.pipe.source.0.source.0 =   ec.es.ESSelection
pop.subpop.0.species.pipe.source.0.source.1 =   ec.es.ESSelection
\endcode

With the high-level pipeline in place, now we'll add parameters for the individual operators.  The 
first set informs `VectorMutationPipeline` that we want to use additive Gaussian mutation, that we 
want mutation to be "bounded" (so that gene values cannot wander outside of their initial allowed 
range of 0.0005–0.5), and that we will always mutation (probability 1.0) and use a mutation width
(Gaussain standard deviation) of 0.5:

\code
pop.subpop.0.species.mutation-type =            gauss
pop.subpop.0.species.mutation-bounded =	        true
pop.subpop.0.species.mutation-prob =            1.0
pop.subpop.0.species.mutation-stdev =           0.5
\endcode

Next we'll define the crossover strategy, here choosing one-point crossover (which chooses one 
point to split individuals at when performing genetic recombination):

\code
pop.subpop.0.species.crossover-type =           one
\endcode

The selection operator `ESSelection` does not require any parameters, but if you use other operators
(such as the popular `TournamentSelection`) they may have parameters.

\note{The `ESSelection` operator and the `MuCommaLambdaBreeder` (or, alternatively, `MuCommaPlusBreeder`
belong to ECJ's evolutionary strategies package, `ecj.es`, and are inteded to be used together.  You 
cannot use `ESSelection` without one of these breeders, because they perform special bookeeping needed for
elitist selection to work.  Likewise, if you want to use other (less elitist) selection operators such as 
`TournamentSelection`, you may prefer to use ECJ's standard `SimpleBreeder` instead of these more complex 
breeders.  See the <a href="https://cs.gmu.edu/~eclab/projects/ecj/manual.pdf">ECJ manual</a> for more
information.}


\subsection Logging

By default, ECJ writes some information about the best individual found at each generation to `stdout`.
If we want more information that this, we'll typically write it to a file.  These two lines activate one 
of ECJ's statistics collection modules and point it to write to the file `./out.stat` in the current directory.

\code
stat =					                        ec.simple.SimpleStatistics
stat.file = 			                        $out.stat
\endcode



















To compile and run the tuning framework we simply type:

\code
make
./launchCARLsimECJ.sh
\endcode

The simulation should run to completion and output the best fitness each generation.

This results in an ECJ out.stat file that we talk about in the next section.


\section tut7s4_output_files 7.4 ECJ Output Files

The output.stat file generated contains the best fitness for that generation along with the four
parameter values associated with that individual. CARLsim users can then use these parameters for
their tuned SNN models.

\see \ref ch10_ecj

*/
